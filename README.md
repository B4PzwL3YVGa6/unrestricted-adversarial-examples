# Unrestricted Adversarial Examples Challenge

In the Unrestricted Adversarial Examples Challenge, attackers submit arbitrary adversarial inputs, and defenders are expected to assign low confidence to difficult inputs while retaining high confidence and accuracy on a clean, unambiguous test set. 

You can learn more about the motivation and structure of the contest in our recent paper:

**Unrestricted Adversarial Examples**<br>
*Authors*<br>
[http://arxiv.org/](http://arxiv.org/)

This repository contains code for the warm-up to the challenge, as well as [the public proposal for the contest](https://github.com/google/unrestricted-adversarial-examples/blob/master/contest_proposal.md). 

![image](https://user-images.githubusercontent.com/306655/44686400-f0b74800-aa02-11e8-8967-fa354244813f.png)


  
## Warm-up phase
### <a name="leaderboard"></a>Leaderboard


| Defense               | Submitted by  | Spatial acc.<br>at 80% cov. | SPSA acc.<br>at 80% cov. | L2-ball acc.<br>at 80% cov. |  Submission Date |
| --------------------- | ------------- | ------------ |--------------- |--------------- | --------------- |
| [Worst-of-10-Spatial Baseline](#)  |  ?? |    **??**    |     **??**   |     **??**     |  Aug 28th, 2018 |
| [Undefended ResNet Baseline](https://github.com/google/unrestricted-adversarial-examples/tree/master/unrestricted_advex/pytorch_resnet_baseline)   |  Google Brain   |    0.0%    |     0.0%    |     0.0%     |  Aug 27th, 2018 |


We include three attacks in the warm-up phase of the challenge

- 1000 Linfinity-ball adversarial examples generated by SPSA
- 1000 spatial adversarial examples (via grid search)
- 100 L2-ball adversarial examples generated by a decision-only attack

### Evaluating a defense

##### Setup
```bash
git clone git@github.com:google/unrestricted-adversarial-examples.git
cd unrestricted-adversarial-examples

pip install -e tcu-images
pip install -e .
```

Confirm that the baseline defense runs correctly for you by running a model. It should print scores that match the leaderboard above.
```bash
python baselines/pytorch_resnet_baseline/train.py
```

##### Implementing a defense

##### To be evaluated against our fixed warm-up attacks, your defense must do all of the following

- [ ] Accept batches of N images (passed into the constructor for the model)
- [ ] For each image return two scalar logits between `[-inf, inf]`. These correspond to the likelihood the image corresponds to each of the two classes (e.g., for TCU-Images, the bird and bicycle class)
- [ ] Maintain a throughput of at least **100 images per second** when evaluated on a P100 GPU on TCU-Images, and XXXX images per second on TCU-MNIST

##### Your defense will be evaluated with the following mechanism

- A test dataset is passed through the model and converted to logits
- Confidence is defined as `max(bicycle_logit, bird_logit)` The 20% of images that resulted in logits with the lowest confidence are abstained on by the model and are discarded.
- The modelâ€™s score is the accuracy on points that were not abstained on.

## Contest phase

The contest phase will begin after the warm-up attacks have been conclusively solved. We have published the [contest proposal](https://github.com/google/unrestricted-adversarial-examples/blob/master/contest_proposal.md) and are soliciting feedback from the community.

## Authors 
