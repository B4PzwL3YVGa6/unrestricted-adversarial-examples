# Unrestricted Adversarial Examples Challenge

In the Unrestricted Adversarial Examples Challenge, attackers submit arbitrary adversarial inputs, and defenders are expected to assign low confidence to difficult inputs while retaining high confidence and accuracy on a clean, unambiguous test set. 

You can learn more about the motivation and structure of the contest in our recent paper:

**Unrestricted Adversarial Examples**<br>
*Authors*<br>
[http://arxiv.org/](http://arxiv.org/)

This repository contains code for the warm-up to the challenge, as well as [the public proposal for the contest](https://github.com/google/unrestricted-adversarial-examples/blob/master/contest_proposal.md). 

![image](https://user-images.githubusercontent.com/306655/44686400-f0b74800-aa02-11e8-8967-fa354244813f.png)

You can see a more detailed structure of the repository below:

* `tcu-images` - The dataset used throughout the challenge
* `unrestricted_advex` - A library used for the evaluation of defenses
* `contest_proposal.md` - Details of the contest portion of the challenge
* `baselines` - Starter code to help you get started with training defenses
  
## <a name="leaderboard"></a>Leaderboard (Warm-up phase)

| Defense               | Submitted by  | SPSA acc@80% | Spatial acc@80% | Submission Date |
| --------------------- | ------------- | ------------ |--------------- | --------------- |
| [PGD-trained Baseline](#)  |  ?? |    **??**    |     **??**     |  Aug 28th, 2018 |
| [Undefended ResNet Baseline](https://github.com/google/unrestricted-adversarial-examples/tree/master/unrestricted_advex/pytorch_resnet_baseline)   |  Google Brain   |    0.0%    |     0.0%     |  Aug 27th, 2018 |


## Benchmarking your defense on the warm-up

#### To be evaluated against our fixed warm-up attacks, your defense must do all of the following

- [ ] Accept batches of N images (passed into the constructor for the model)
- [ ] For each image return two scalar logits between `[-inf, inf]`. These correspond to the likelihood the image corresponds to each of the two classes (e.g., for TCU-Images, the bird and bicycle class)
- [ ] Maintain a throughput of at least **100 images per second** when evaluated on a P100 GPU on TCU-Images, and XXXX images per second on TCU-MNIST

#### Your defense will be evaluated with the following mechanism

- A test dataset is passed through the model and converted to logits
- Confidence is defined as `max(bicycle_logit, bird_logit)` The 20% of images that resulted in logits with the lowest confidence are abstained on by the model and are discarded.
- The modelâ€™s score is the accuracy on points that were not abstained on.

#### Your defense will be evaluated on the following four datasets

- 1000 clean data points from TCU-images
- 1000 Linfinity-ball adversarial examples generated by SPSA
- 1000 spatial adversarial examples (via grid search)
- 100 L2-ball adversarial examples generated by a decision-only attack
